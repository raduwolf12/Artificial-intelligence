{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    #A building block. Each layer is capable of performing two things:\n",
    "    #- Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    #Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "    def backward(self, input, grad_output):\n",
    "        # Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n",
    "        \n",
    "        # d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n",
    "        \n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Apply elementwise ReLU to [batch, input_units] matrix\n",
    "        relu_forward = np.maximum(0,input)\n",
    "        return relu_forward\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        # Compute gradient of loss w.r.t. ReLU input\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        # A dense layer is a layer which performs a learned affine transformation:\n",
    "        # f(x) = <W*x> + b\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(loc=0.0, \n",
    "                                        scale = np.sqrt(2/(input_units+output_units)), \n",
    "                                        size = (input_units,output_units))\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        # Perform an affine transformation:\n",
    "        # f(x) = <W*x> + b\n",
    "        \n",
    "        # input shape: [batch, input_units]\n",
    "        # output shape: [batch, output units]\n",
    "        \n",
    "        return np.dot(input,self.weights) + self.biases\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.dot(input.T, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        \n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "%i format: a number is required, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6e828f2b5265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Label: %i\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: %i format: a number is required, not numpy.ndarray"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, List, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def load_file(path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Loads the data from the file stored at :param path: and returns the \n",
    "    input values and the class labels.\n",
    "    :param path: path of a CVS file with data\n",
    "    :return: a tuple containing the input matrix of shape (n, p) and a line \n",
    "    vector with the m class labels in {0, ..., 9}\n",
    "    \"\"\"\n",
    "    # citire date sin fisierul dat de path\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    X =  df.values[:,1:]\n",
    "    X =X.reshape((df.shape[1])-1,df.shape[0])\n",
    "    y = df.values[:,0].reshape(1,len(df))\n",
    "    assert X.ndim ==  2, 'Matrix required for input values'\n",
    "    assert y.ndim == 2, 'Column matrix required for labels'\n",
    "    assert y.shape == (1, X.shape[1]), 'Same number of lines is required'\n",
    "    return X, y\n",
    "path_train = './lab4/data/mnist_train.csv'\n",
    "path_test = './lab4/data/mnist_test.csv'\n",
    "\n",
    "X_train, y_train = load_file(path_train)\n",
    "\n",
    "X_test, y_test = load_file(path_test)\n",
    "\n",
    "## Let's look at some example\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
